{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa8eb8-9437-476d-a568-6bbcfe647b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# FLNSim: Simulating federated learning over LoRa networks\n",
    "#\n",
    "# Copyright (c) 2025 Anshika Singh and Siddhartha S. Borkotoky <siddhartha.borkotoky@gmail.com>\n",
    "# All rights reserved for original contributions and modifications.\n",
    "#\n",
    "# This simulator includes and adapts code from the following projects:\n",
    "#\n",
    "# • Flower (flwr) – Federated Learning Framework\n",
    "#   Copyright 2020–2025 Flower Labs GmbH (formerly ADAP/University of Oxford).\n",
    "#   Licensed under the Apache License, Version 2.0.\n",
    "#   Original source: https://github.com/adap/flower \n",
    "#\n",
    "# • LoRaSim – Discrete-event LoRa network simulator\n",
    "#   Copyright 2016–2017 Thiemo Voigt & Martin Bor.\n",
    "#   Licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0).\n",
    "#   Original source: https://github.com/paafam/LoRaSim  \n",
    "#\n",
    "# Note: Portions of this code have been modified from their original implementations.\n",
    "#       We retain all original notices, copyright statements, and license texts.\n",
    "#\n",
    "# This work is licensed under the Creative Commons Attribution 4.0 International License. \n",
    "# To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Overview:\n",
    "\n",
    "# The program simulates federated learning sessions (a CNN-based digit classification task on the MNIST dataset) \n",
    "# assuming that the server is connected to a LoRa gateway and the clients are LoRa end devices.  \n",
    "\n",
    "# The simulation outputs are as follows (written to a file output.txt): \n",
    "    # the accuracy of the global model at the end of each round \n",
    "    # the round completion time, defined as the time when all sampled clients finish transmitting their local updates\n",
    "    # the downlink airtime, which is the total time the server spends transmitting in a round\n",
    "    # the \\cumulative uplink airtime, defined as the total time all sampled clients spend transmitting in a round, aggregated across clients\n",
    "  \n",
    "# The input parameters are \n",
    "    # num_sessions: Number of FL sessions to simulate (final results will be averaged over the sessions)\n",
    "    # rounds_per_session: Number of training rounds per session   \n",
    "    # num_clients: Number of clients\n",
    "    # network_radius: Network radius in meters (clients will be randomly deployed within a circle of this radius, with the server at the origin)\n",
    "    # spreading_factor_FL: LoRa spreading factor to be used for transmitting FL updates\n",
    "    # use_quantization: When set to True, model parameter values will be quantized\n",
    "    # quantization_bits: Number of quantization bits (supports 1, 2, and 4 bits)\n",
    "    # use_sparsity: When set to True, model parameters values are sparsified by setting values below a threshold to zero, and if so, what threshold to use\n",
    "    # sparsity_threshold: Threshold for sparsification\n",
    "    # apply_zlib: Whether to compress the updates prior to transmission\n",
    "    # LoRa_Class: The class of LoRaWAN operation to be used (supports class B and C)\n",
    "    # ping_slot_duration: Ping slot periodicity (only for Class B)\n",
    "    # Sampling ratio (the fraction of clients chosen for update exchange in each round)\n",
    "    # FEC_rate: Rate of the FEC applied to the fragments\n",
    "    # duty_cycle_percentage: Maximum permitted value for transmitter duty cycle (in percentage)\n",
    "    # interferer_intensity: Number of interferers per sq. meter (assumes a Poisson Point Process)\n",
    "    # full_simulation: Whether to actually simulate LoRa tranmissions or apply analytical approximations\n",
    "    # mean_interf_interval: Mean of the time interval (in milliseconds) between two consecutive packets from an interferer\n",
    "    # interf_payload_range: Range of values for interfering frame payload (an interfering frame carries a payload that is chosen uniformly at random from within this range)             \n",
    "    # LoRa_channel_code: Defines the channel coding rate for LoRa frames\n",
    "    # bandwidth: Transmission bandwidth (kHz)\n",
    "    # Ptx: power of the transmitted signal in dBm\n",
    "    # path_loss_exponent: Models the exponential decaying of signals\n",
    "    # num_channels: Number of non overlapping frequency bands available to LoRa transmitters\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455b38f-ef15-4036-94db-86b58d570566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tenseal\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install simpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e65942-0553-46c6-997b-8f0e9781c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b6f19-d451-4393-bb68-6d8d717faae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import copy\n",
    "import simpy\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.patches import Rectangle\n",
    "import operator as op\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd6d7f-abe9-4a5f-b6a9-aa1e796298b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402e7ea-38da-4633-a581-adb79e1e3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef682fc-4dab-43ac-a8d4-08bb85be6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import tenseal as ts\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import zlib\n",
    "from scipy.special import gammainc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import gammainc, comb\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88ec61-5797-45ee-9a00-dd06e292758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84846ef4-8705-404f-b790-9fe91cb4c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flower model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)  # 1 input channel\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # MNIST input -> 28x28 -> conv/pool -> 4x4\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fcedb-c64e-47ae-b8b8-42ce4a5098a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr_datasets import FederatedDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_datasets(num_clients: int):\n",
    "    # Load FederatedDataset for MNIST\n",
    "    fds = FederatedDataset(dataset=\"mnist\", partitioners={\"train\": num_clients})\n",
    "\n",
    "    # Standard MNIST transform\n",
    "    pytorch_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
    "        return batch\n",
    "\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    datasets = []\n",
    "\n",
    "    # Loop over each client partition\n",
    "    for cid in range(num_clients):\n",
    "        partition = fds.load_partition(cid)\n",
    "        partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "        partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "\n",
    "        trainloader = DataLoader(partition_train_test[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
    "        valloader = DataLoader(partition_train_test[\"test\"], batch_size=BATCH_SIZE)\n",
    "\n",
    "        trainloaders.append(trainloader)\n",
    "        valloaders.append(valloader)\n",
    "        datasets.append(partition)  # Optional: Store raw partition for reference\n",
    "\n",
    "    # Load and transform global test set\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return trainloaders, valloaders, testloader, datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223442c-fd75-4cb2-9d1b-1ffbc617d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Global model\n",
    "global_model = Net().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f3eb8-f0f2-4168-a728-571541f802dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, accuracy, local_loss, epochs: int):\n",
    "  \"\"\"Train the network on the training set.\"\"\"\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr= 0.01)\n",
    "  params = list(net.parameters())\n",
    "  net.train()\n",
    "  for epoch in range(epochs):\n",
    "      correct, total, epoch_loss = 0, 0, 0.0\n",
    "      for batch in trainloader:\n",
    "          images, labels = batch[\"image\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = net(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          # Metrics\n",
    "          epoch_loss += loss\n",
    "          total += labels.size(0)\n",
    "          correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "      epoch_loss /= len(trainloader.dataset)\n",
    "      epoch_acc = correct / total\n",
    "      accuracy.append(epoch_acc)\n",
    "      local_loss.append(epoch_loss.item())\n",
    "      #print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c816d4b-4d2d-4fe4-9f56-3d53cf515048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified test code \n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"image\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d5462-bc7e-48fd-a0e1-20d089eb5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pack 4-bit values into bytes\n",
    "def pack_4bit(tensor):\n",
    "    \"\"\"Pack 4-bit values (0–15) into bytes (2 values per byte).\"\"\"\n",
    "    tensor = tensor.astype(np.uint8)\n",
    "    assert np.all(tensor < 16), \"All values must be 4-bit (0–15)\"\n",
    "\n",
    "    packed = np.zeros((tensor.size + 1) // 2, dtype=np.uint8)\n",
    "\n",
    "    for i in range(tensor.size):\n",
    "        byte_idx = i // 2\n",
    "        if i % 2 == 0:\n",
    "            # Lower 4 bits\n",
    "            packed[byte_idx] |= (tensor[i] & 0xF)\n",
    "        else:\n",
    "            # Upper 4 bits\n",
    "            packed[byte_idx] |= (tensor[i] & 0xF) << 4\n",
    "\n",
    "    return packed\n",
    "\n",
    "# Helper function to unpack 4-bit values from bytes\n",
    "def unpack_4bit(packed, original_shape):\n",
    "    \"\"\"Unpack 4-bit values from bytes to original shape.\"\"\"\n",
    "    unpacked = np.zeros(np.prod(original_shape), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(unpacked.size):\n",
    "        byte_idx = i // 2\n",
    "        if i % 2 == 0:\n",
    "            # Lower 4 bits\n",
    "            unpacked[i] = packed[byte_idx] & 0xF\n",
    "        else:\n",
    "            # Upper 4 bits\n",
    "            unpacked[i] = (packed[byte_idx] >> 4) & 0xF\n",
    "    \n",
    "    return unpacked.reshape(original_shape)\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to pack 2-bit values into bytes\n",
    "def pack_2bit(tensor):\n",
    "    \"\"\"Pack 2-bit values (0-3) into bytes (4 values per byte).\"\"\"\n",
    "    tensor = tensor.astype(np.uint8)\n",
    "    packed = np.zeros((tensor.size + 3) // 4, dtype=np.uint8)\n",
    "    for i in range(tensor.size):\n",
    "        byte_idx = i // 4\n",
    "        bit_idx = (i % 4) * 2\n",
    "        packed[byte_idx] |= (tensor[i] & 0x3) << bit_idx\n",
    "    return packed\n",
    "\n",
    "# Helper function to unpack 2-bit values from bytes\n",
    "def unpack_2bit(packed, original_shape):\n",
    "    \"\"\"Unpack 2-bit values from bytes to original shape.\"\"\"\n",
    "    unpacked = np.zeros(np.prod(original_shape), dtype=np.uint8)\n",
    "    for i in range(unpacked.size):\n",
    "        byte_idx = i // 4\n",
    "        bit_idx = (i % 4) * 2\n",
    "        unpacked[i] = (packed[byte_idx] >> bit_idx) & 0x3\n",
    "    return unpacked.reshape(original_shape)\n",
    "\n",
    "def pack_1bit(tensor):\n",
    "    tensor = tensor.astype(np.uint8)\n",
    "    packed = np.zeros((tensor.size + 7) // 8, dtype=np.uint8)\n",
    "    for i in range(tensor.size):\n",
    "        byte_idx = i // 8\n",
    "        bit_idx = i % 8\n",
    "        packed[byte_idx] |= (tensor[i] & 0x1) << bit_idx\n",
    "    return packed\n",
    "\n",
    "def unpack_1bit(packed, original_shape):\n",
    "    \"\"\"Unpack 1-bit values from bytes to original shape.\"\"\"\n",
    "    unpacked = np.zeros(np.prod(original_shape), dtype=np.uint8)\n",
    "    for i in range(unpacked.size):\n",
    "        byte_idx = i // 8\n",
    "        bit_idx = i % 8\n",
    "        unpacked[i] = (packed[byte_idx] >> bit_idx) & 0x1\n",
    "    return unpacked.reshape(original_shape)\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in KB. Assumes 4-byte representation of each non-zero parameter, with no further quantization or sparsification\n",
    "    \"\"\"\n",
    "    # Count non-zero parameters\n",
    "    non_zero_params = 0\n",
    "    for param in model.parameters():\n",
    "        non_zero_params += param.numel()\n",
    "        \n",
    "    # Each parameter has a 4-byte representation\n",
    "    size_bytes = non_zero_params * 4\n",
    "    \n",
    "    return size_bytes / 1024  # Convert to KB\n",
    "\n",
    "\n",
    "def sparsify(update_dict, threshold = 0.001):\n",
    "    \"\"\" Sets parameter values below the threshold to zero \"\"\"\n",
    "    sparsity_levels = []\n",
    "    \n",
    "    for tensor in update_dict.values():\n",
    "        mask = torch.abs(tensor) < threshold\n",
    "        sparsity_levels.append(mask.float().mean().item())\n",
    "        tensor[mask] = 0\n",
    "    \n",
    "    avg_sparsity = statistics.mean(sparsity_levels) if sparsity_levels else 0.0\n",
    "    return update_dict, avg_sparsity\n",
    "\n",
    "\n",
    "def quantize(tensor, quantization_bits = 2):\n",
    "    tensor_np = tensor.cpu().numpy()\n",
    "    shape = tensor_np.shape\n",
    "    \n",
    "    if quantization_bits in [1,2,4]:\n",
    "        min_val, max_val = tensor_np.min(), tensor_np.max()\n",
    "        levels = (2**quantization_bits) - 1\n",
    "\n",
    "        if max_val == min_val:\n",
    "            quantized_tensor = np.zeros_like(tensor_np, dtype=np.uint8)\n",
    "        else:\n",
    "            quantized_tensor = np.round((tensor_np - min_val) / (max_val - min_val) * levels).astype(np.uint8)\n",
    "            \n",
    "        data = quantized_tensor.flatten()\n",
    "\n",
    "        if quantization_bits == 4:\n",
    "            data = pack_4bit(data)\n",
    "        elif quantization_bits == 2:\n",
    "            data = pack_2bit(data)\n",
    "        elif quantization_bits == 1:\n",
    "            data = pack_1bit(data)\n",
    "            \n",
    "        data_bytes = data.tobytes()\n",
    "        tensor_size = np.prod(shape) * (quantization_bits / 8)\n",
    "        min_max = (min_val, max_val)\n",
    "    else:\n",
    "        data_bytes = tensor_np.tobytes()\n",
    "        tensor_size = np.prod(shape) * 4\n",
    "        min_max = None\n",
    "    return data_bytes, shape, min_max, tensor_size\n",
    "    \n",
    "\n",
    "def compress_update(model, reference_model, use_sparsity, use_quantization, apply_zlib, sparsity_threshold, quantization_bits):\n",
    "    \"\"\"Compress model update (delta) dictionary\"\"\"\n",
    "    compressed_data = {}\n",
    "    original_shapes = {}\n",
    "    original_min_max = {}\n",
    "    uncompressed_size = 0\n",
    "\n",
    "    model_dict = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        model_param = next(p for n, p in model.named_parameters() if n == name)\n",
    "        if reference_model is None:\n",
    "            model_dict[name] = model_param.data\n",
    "        else:\n",
    "            ref_model_param = next(p for n, p in reference_model.named_parameters() if n == name)\n",
    "            model_dict[name] = model_param.data - ref_model_param.data\n",
    "\n",
    "    if use_sparsity:\n",
    "        model_dict, sparsity = sparsify(model_dict, threshold = sparsity_threshold)\n",
    "    \n",
    "    for key, tensor in model_dict.items():\n",
    "        data_bytes, shape, min_max, tensor_size = quantize(tensor, quantization_bits if use_quantization else 0)\n",
    "        original_shapes[key] = shape\n",
    "        if min_max is not None:\n",
    "            original_min_max[key] = min_max\n",
    "        uncompressed_size += tensor_size\n",
    "        \n",
    "        if apply_zlib:\n",
    "            compressed_data[key] = zlib.compress(data_bytes, level = 9)\n",
    "        else:\n",
    "            compressed_data[key] = data_bytes\n",
    "    \n",
    "    compressed_size = sum(len(v) for v in compressed_data.values()) / 1024\n",
    "    compression_ratio = uncompressed_size / (compressed_size * 1024) if compressed_size > 0 and apply_zlib else 1.0\n",
    "    \n",
    "    return compressed_data, compressed_size, original_shapes, original_min_max, compression_ratio\n",
    "\n",
    "def decompress_update(compressed_data, original_shapes, original_min_max, use_quantization=False, apply_zlib=False, quantization_bits=2):\n",
    "    \"\"\"Decompress model update (delta) dictionary\"\"\"\n",
    "    update_dict = {}\n",
    "    for key, data in compressed_data.items():\n",
    "        data_bytes = zlib.decompress(data) if apply_zlib else data\n",
    "        tensor = dequantize(data_bytes, original_shapes[key], original_min_max.get(key), quantization_bits if use_quantization else 0)\n",
    "        update_dict[key] = tensor\n",
    "    return update_dict\n",
    "\n",
    "\n",
    "def dequantize(data_bytes, shape, min_max, quantization_bits=0):\n",
    "    expected_elements = np.prod(shape)\n",
    "\n",
    "    if quantization_bits in [1,2,4]:\n",
    "        if quantization_bits == 2:\n",
    "            expected_bytes = (expected_elements + 3) // 4\n",
    "            if len(data_bytes) != expected_bytes:\n",
    "                raise ValueError(f\"Buffer size mismatch for {key}: got {len(data_bytes)} bytes, expected {expected_bytes}\")\n",
    "            packed_tensor = np.frombuffer(data_bytes, dtype=np.uint8)\n",
    "            quantized_tensor = unpack_2bit(packed_tensor, shape)\n",
    "        elif quantization_bits == 1:\n",
    "            expected_bytes = (expected_elements + 7) // 8\n",
    "            if len(data_bytes) != expected_bytes:\n",
    "                raise ValueError(f\"Buffer size mismatch for {key}: got {len(data_bytes)} bytes, expected {expected_bytes}\")\n",
    "            packed_tensor = np.frombuffer(data_bytes, dtype=np.uint8)\n",
    "            quantized_tensor = unpack_1bit(packed_tensor, shape)\n",
    "        else:\n",
    "            expected_bytes = (expected_elements + 1) // 2\n",
    "            if len(data_bytes) != expected_bytes:\n",
    "                raise ValueError(f\"Buffer size mismatch for {key}: got {len(data_bytes)} bytes, expected {expected_bytes}\")\n",
    "            packed_tensor = np.frombuffer(data_bytes, dtype=np.uint8)\n",
    "            quantized_tensor = unpack_4bit(packed_tensor, shape)\n",
    "        \n",
    "        if min_max is not None:\n",
    "            min_val, max_val = min_max\n",
    "            levels = (2 ** quantization_bits) - 1\n",
    "            if max_val == min_val:\n",
    "                tensor_np = np.full(shape, min_val, dtype=np.float32)\n",
    "            else:\n",
    "                tensor_np = (quantized_tensor / levels) * (max_val - min_val) + min_val\n",
    "        else:\n",
    "            tensor_np = quantized_tensor.astype(np.float32)\n",
    "    else:\n",
    "        expected_bytes = expected_elements * 4\n",
    "        if len(data_bytes) != expected_bytes:\n",
    "            raise ValueError(f\"Buffer size mismatch for {key}: got {len(data_bytes)} bytes, expected {expected_bytes}\")\n",
    "        tensor_np = np.frombuffer(data_bytes, dtype=np.float32).reshape(shape)\n",
    "    \n",
    "    return torch.tensor(tensor_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f3c83-8068-424b-9476-6c2f76140eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a frame/fragment delivery probability Pd, this function evaluates the probability that k out of n frames are successully delivered\n",
    "import math\n",
    "from scipy.special import gammainc\n",
    "from scipy.stats import binom, poisson\n",
    "\n",
    "def success_prob_k_of_n(Pd, k, n):                          \n",
    "    \n",
    "    j = np.arange(k, n + 1)\n",
    "\n",
    "    # Use binomial distribution\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        MDP = np.sum(comb(n,j)*((Pd**j)*((1-Pd)**(n-j))))\n",
    "\n",
    "    # If the binomial distribution evaluates to NaN, try the Poisson approximation\n",
    "    if math.isnan(MDP):\n",
    "        if Pd < 0.5:\n",
    "            MDP = np.sum(poisson.pmf(j, n * Pd))\n",
    "        else:\n",
    "            j1 = np.arange(0, n-k+1)\n",
    "            MDP = np.sum(poisson.pmf(j1, n * (1-Pd)))\n",
    "            \n",
    "    return MDP\n",
    "\n",
    "# LoRa MTU (Maximum Transmission Unit)    \n",
    "def LoRa_MTU(sf):                                             \n",
    "    return 222 if sf in [7, 8] else 115 if sf == 9 else 51  # For SF 10, 11, 12\n",
    "\n",
    "\n",
    "# Frame length computation functions\n",
    "def LoRa_frame_duration(sf, payload_bytes):\n",
    "    npr = 8\n",
    "    h = 1  # Optional header\n",
    "    c = 1  # Coding rate\n",
    "    bw = 125 * 10**3  # Bandwidth in Hz\n",
    "    return ((npr + 4.25) * (2**sf / bw) +\n",
    "            (8 + max(np.ceil((2 * payload_bytes - sf - 5 * h + 11) / (sf - 2*(sf >= 11))) * (c + 4), 0)) * (2**sf / bw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d5cb9-fd69-46e8-80de-5b1a463dc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the probability that a frame/fragment transmitted over a \n",
    "# given distance is succesfully delivered\n",
    "# Analysis based on: S. S. Borkotoky, \"Balancing the Energy Consumption and Latency of Over-the-Air Firmware Updates in LoRaWAN,\" in IEEE Transactions on Industrial Informatics, doi: 10.1109/TII.2025.3563539.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "def compute_fragment_delivery_prob(receiver_distances, sf, fragment_size):\n",
    "    \n",
    "    \n",
    "    # Inteference characterstics\n",
    "    mean_interval = mean_interf_interval  # Mean interval between two consecutive frames from an interferer (ms)\n",
    "    avg_interferer_payload = int(np.mean(interf_payload_range))  # Payload per interferer message (bytes)\n",
    "    lambda_interf = 1 / mean_interval  # Interfering frames per millisecond\n",
    "        \n",
    "    # Result storage for model delivery probabilities\n",
    "    frag_delivery_probs = []\n",
    "\n",
    "    # Iterate over each receiver\n",
    "    for dist in receiver_distances:\n",
    "        \n",
    "        # Adjust payload size based on MTU\n",
    "        payload_size = fragment_size\n",
    "\n",
    "        # Compute frame success probability\n",
    "        a_vals = np.arange(0.005, 10.005, 0.001)  # Fading coefficient range\n",
    "        FSP_cond_a = np.zeros_like(a_vals)\n",
    "\n",
    "        for j, a in enumerate(a_vals):\n",
    "            f1 = 0\n",
    "            for sf_interf in range(7, 13):\n",
    "                xi = 10**(0.1 * cap_thresh[sf - 7, sf_interf - 7])\n",
    "                des_frame_length = airtime(sf, 1, payload_size, 125)\n",
    "                interf_frame_length = airtime(sf_interf, 1, avg_interferer_payload, 125)              \n",
    "                vuln_wind = des_frame_length + interf_frame_length\n",
    "                c_ij = (lambda_interf / num_channels) * vuln_wind\n",
    "                beta_j = xi**(-1) * a * dist**(-path_loss_exp)\n",
    "                gammadiff = gammainc(2 / path_loss_exp, beta_j * (simulation_radius**path_loss_exp))  #  path_loss_exp function\n",
    "                f1 += c_ij * gammadiff / (beta_j**(2 / path_loss_exp))\n",
    "\n",
    "            QI = (1 / 6) * (2 / (path_loss_exp * simulation_radius**2)) * f1\n",
    "            succ_interference = (1 - QI)**avg_num_interferers\n",
    "\n",
    "            fsensitivity_dBm = sensitivity[sf - 7]\n",
    "            rx_pow_dBm = Ptx - Lpld0 + GtGr + 10 * np.log10(a) - 10 * path_loss_exp * np.log10(dist / d0)\n",
    "            succ_fading = rx_pow_dBm >= fsensitivity_dBm\n",
    "\n",
    "            FSP_cond_a[j] = succ_fading * succ_interference * np.exp(-a)\n",
    "\n",
    "        \n",
    "        P_frag = trapz(FSP_cond_a, a_vals)\n",
    "        #Probability of receiving at least k out of n fragments\n",
    "        frag_delivery_probs.append(P_frag)\n",
    "        # print(P_frag)\n",
    "\n",
    "    return frag_delivery_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ba534-79d2-4115-9351-daf7f09cd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the probability that an update is successfully transfered to the recipient(s)\n",
    "\n",
    "def compute_model_delivery_prob(transmitter_coordinates, receiver_coordinates, sf, frag_size, k, r):\n",
    "    \n",
    "    model_delivery_probs = []\n",
    "    receiver_distances = []\n",
    "\n",
    "    for rx_coordinates in receiver_coordinates:\n",
    "        dist = np.sqrt((transmitter_coordinates[0] - rx_coordinates[0])**2 + (transmitter_coordinates[1] - rx_coordinates[1])**2)\n",
    "        receiver_distances.append(dist)\n",
    "    \n",
    "    fragment_delivery_probs = compute_fragment_delivery_prob(receiver_distances, sf, frag_size)\n",
    "\n",
    "    #print(\"fragment_delivery_probs, receiver_distances, sf, frag_size: \",fragment_delivery_probs, receiver_distances, sf, frag_size)\n",
    "    \n",
    "    n = k + r\n",
    "\n",
    "    for P_frag in fragment_delivery_probs:\n",
    "        P_model = success_prob_k_of_n(P_frag, k, n)\n",
    "        model_delivery_probs.append(P_model)\n",
    "\n",
    "    return model_delivery_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4898a-4017-4d23-847e-edf7cc974c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def set_parameters_to_clients(net: nn.Module, parameters: Union[List[np.ndarray], nn.Module]):\n",
    "    \"\"\"Set the parameters of net to the provided parameters or model.\"\"\"\n",
    "    if isinstance(parameters, nn.Module):\n",
    "        parameters = [val.cpu().numpy() for _, val in parameters.state_dict().items()]\n",
    "    \n",
    "    # Check if model has parameters\n",
    "    try:\n",
    "        device = next(net.parameters()).device\n",
    "    except StopIteration:\n",
    "        raise ValueError(\"Model has no parameters to set.\")\n",
    "    \n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v).to(device) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8310c-39d0-40d4-b211-02513e8b7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_updates(updates):\n",
    "    \"\"\"Average decompressed updates\"\"\"\n",
    "    if not updates:\n",
    "        return {}\n",
    "    \n",
    "    aggregated = {}\n",
    "    for key in updates[0].keys():\n",
    "        tensors = [update[key] for update in updates]\n",
    "        aggregated[key] = torch.stack(tensors).mean(dim=0)\n",
    "    \n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1af62-2777-4f42-b901-15c883ecbc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# check for collisions at base station\n",
    "# Note: called before a packet (or rather node) is inserted into the list\n",
    "#\n",
    "\n",
    "def checkcollision(packet):\n",
    "    col = 0 # flag needed since there might be several collisions for a packet\n",
    "    # lost frames don't collide\n",
    "    # (they go undetected since the received power is below the receiver sensitivity)\n",
    "    if packet.detection_failure:\n",
    "        return 0\n",
    "    if packetsAtDest[packet.rcvr_id]:\n",
    "        # see if another frame currently being recieved is capable of causing the loss of the frame\n",
    "        # do the above for every other frame that are currently being received\n",
    "        for other in packetsAtDest[packet.rcvr_id]:\n",
    "            if other.id != packet.nodeid:\n",
    "                if packet.freq == other.packet[packet.rcvr_id].freq:                   \n",
    "                    col = timingandpowercheck(packet, other.packet[packet.rcvr_id])                        \n",
    "        return col\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e57202-cf32-4bd9-94f5-0f3c1272782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this function checks if the freshly arrived frame (p1) is lost due to a frame p2 that\n",
    "# was already being received by the gateway. Returns 1 if frame loss conditions are satisfied\n",
    "#\n",
    "def timingandpowercheck(p1, p2):\n",
    "    #\n",
    "    # Is p1 (which just arrived) lost due to p2 (which arrived earlier)?\n",
    "    #\n",
    "    \n",
    "    # Capture-effect threshold \n",
    "    powerThreshold = cap_thresh[p1.sf - 7, p2.sf - 7] # dB\n",
    "\n",
    "    # no. of preamble symbols in a frame\n",
    "    Npream = 8\n",
    "\n",
    "    # we can lose at most (Npream - 5) * Tsym of p1's preamble\n",
    "    Tpreamb = 2**p1.sf/(1.0*p1.bw) * (Npream - 5)\n",
    "\n",
    "    # the time at which p2 will end\n",
    "    p2_end = p2.addTime + p2.rectime\n",
    "\n",
    "    # ending time for the critical section of p1\n",
    "    p1_cs = env.now + Tpreamb\n",
    "\n",
    "    # Is p2 a strong inteferer from p1's perspective?    \n",
    "    p2_is_strong_interferer = (p1.rssi - p2.rssi) < powerThreshold\n",
    "\n",
    "    # check whether p2 corrupts p1's critical section\n",
    "    if p2_is_strong_interferer and p1_cs < p2_end:\n",
    "        # p1 was lost\n",
    "        p1.collided = 1\n",
    "            \n",
    "    #\n",
    "    # Is p2 lost due to p1?\n",
    "    #\n",
    "    \n",
    "    # Is p1 a strong inteferer from p2's perspective?    \n",
    "    p1_is_strong_interferer = (p2.rssi - p1.rssi) < powerThreshold\n",
    "\n",
    "    # If p1 is a strong interferer, then p2 is lost\n",
    "    if p1_is_strong_interferer:        \n",
    "        p2.collided = 1\n",
    "                \n",
    "    return p1.collided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840facdc-e1b5-4fa4-b121-31dc040a9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this function computes the airtime of a packet\n",
    "# according to LoraDesignGuide_STD.pdf\n",
    "#\n",
    "\n",
    "def airtime(sf,cr,pl,bw):\n",
    "    H = 1        # implicit header disabled (H=0) or not (H=1)\n",
    "    DE = 0       # low data rate optimization enabled (=1) or not (=0)\n",
    "    Npream = 8   # number of preamble symbol (12.25  from Utz paper)\n",
    "\n",
    "    if bw == 125 and sf in [11, 12]:\n",
    "        # low data rate optimization mandated for BW125 with SF11 and SF12\n",
    "        DE = 1\n",
    "    if sf == 6:\n",
    "        # can only have implicit header with SF6\n",
    "        H = 1\n",
    "\n",
    "    Tsym = (2.0**sf)/bw\n",
    "    Tpream = (Npream + 4.25)*Tsym\n",
    "    payloadSymbNB = 8 + max(math.ceil((8.0*pl-4.0*sf+28+16-20*H)/(4.0*(sf-2*DE)))*(cr+4),0)\n",
    "    Tpayload = payloadSymbNB * Tsym\n",
    "    return Tpream + Tpayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd9717-28ae-4563-bfb4-6a4b2456259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this function creates a receiver\n",
    "#\n",
    "class myReceiver():\n",
    "    def __init__(self, id, coordinates):\n",
    "        \n",
    "        self.id = id\n",
    "\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "\n",
    "            \n",
    "        # Buffer contents at Dest\n",
    "        self.buffered_node = []\n",
    "        self.buffered_seq = []\n",
    "\n",
    "        # To keep track of the number of FL fragments received\n",
    "        self.total_rcvd = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1498c07-4092-4f5f-af1a-2bf3d05b4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this function creates a node\n",
    "#\n",
    "\n",
    "class myTransmitter():\n",
    "    def __init__(self, id, coordinates, mean_interf_interval):\n",
    "        global rx_nodes, spreading_factor_FL\n",
    "\n",
    "        self.id = id\n",
    "        self.period = mean_interf_interval\n",
    "        self.tx_type = 1\n",
    "        self.packet = []\n",
    "        self.frame_seqnum = -1\n",
    "\n",
    "\n",
    "        # node 0 is the FL transmitter, others are interferers\n",
    "        if self.id == 0:\n",
    "            self.tx_type = 0 # Periodic traffic\n",
    "            self.period = 0 # Send frames one after the other\n",
    "            self.sf = spreading_factor_FL # Use fixed SF\n",
    "            self.CU = 0 # Do not request ACKs \n",
    "            self.x = coordinates[0]\n",
    "            self.y = coordinates[1]  \n",
    "        else:\n",
    "            self.tx_type = 1 # Exponential traffic\n",
    "            self.period = mean_interf_interval\n",
    "            self.sf = np.random.randint(7, 13) # Use a random SF\n",
    "                        \n",
    "            self.CU = 0 # Do not request ACKs \n",
    "            self.x = coordinates[0]\n",
    "            self.y = coordinates[1]      \n",
    "        \n",
    "        # intialize counters to keep track of how many messages a node has sent\n",
    "        # and how many have been successfully delivered\n",
    "        self.sent = 0\n",
    "        self.delivered = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b9638-522c-4466-9ba3-ff59ffe65d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this function creates a packet (associated with a node)\n",
    "\n",
    "class myPacket():\n",
    "    def __init__(self, nodeid, sf, plen, distance, rcvr_id):\n",
    "        global Ptx\n",
    "        global path_loss_exp\n",
    "        global d0\n",
    "        global Lpld0\n",
    "        global LoRa_channel_code\n",
    "        global bandwidth\n",
    "\n",
    "\n",
    "        # new: base station ID\n",
    "        self.rcvr_id = rcvr_id\n",
    "\n",
    "        # originating node id\n",
    "        self.nodeid = nodeid\n",
    "\n",
    "        # spreading factor, code rate, and bandwidth for the frame\n",
    "        self.sf = sf\n",
    "        self.cr = LoRa_channel_code\n",
    "        self.bw = bandwidth\n",
    "\n",
    "        # payload length\n",
    "        self.pl = plen\n",
    "     \n",
    "        # propagation loss for the frame\n",
    "        propagation_loss = Lpld0 + 10*path_loss_exp*math.log10(distance/d0)\n",
    "\n",
    "        # fading gain for the frame (Rayleigh fading, equivalent to Nakagami-m with m = 1)\n",
    "        nakagami_m = 1  \n",
    "        fading_gain = 10 * math.log10(random.gammavariate(nakagami_m, 1.0/float(nakagami_m)))\n",
    "\n",
    "        # received power in the frame\n",
    "        self.rssi = Ptx + GtGr - propagation_loss + fading_gain\n",
    "\n",
    "        # choose one of the three center frquencies at random\n",
    "        self.freq = random.randint(1, num_channels)\n",
    "\n",
    "        # on-air time for the frame\n",
    "        self.rectime = airtime(self.sf,self.cr,self.pl,self.bw)\n",
    "        \n",
    "        \n",
    "        # flags for collision and detection failure\n",
    "        self.collided = 0\n",
    "        self.processed = 0 \n",
    "        self.detection_failure = 0        \n",
    "\n",
    "        if self.rssi < sensitivity[self.sf - 7]:            \n",
    "            self.detection_failure = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd8650-6210-4223-918e-f6f32bd3c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# transmission event loop, runs for each node\n",
    "# a global list of packet being processed at the gateway is maintained\n",
    "#\n",
    "\n",
    "def transmit(env, node, FL_fragment_size, interf_payload_range, tx_limit, numReceivers, rx_nodes, stop_event):\n",
    "    while True:\n",
    "\n",
    "        global packetSeq\n",
    "\n",
    "        # payload size for the frame \n",
    "        if node.id == 0:\n",
    "            payload_size = FL_fragment_size\n",
    "        else:\n",
    "            payload_size = random.randrange(interf_payload_range[0], interf_payload_range[1])\n",
    "\n",
    "\n",
    "        # Node transmissions  \n",
    "        if node.tx_type == 1:\n",
    "            yield env.timeout(random.expovariate(1.0/float(node.period)))\n",
    "        else:\t        \n",
    "            if node.sent == 0:\n",
    "                yield env.timeout(random.uniform(0, node.period))\n",
    "            yield env.timeout(airtime(node.sf, LoRa_channel_code, payload_size, bandwidth))\n",
    "\n",
    "        # what will be the ending time of the current packet?\n",
    "        pkt_end_time = env.now + airtime(node.sf, LoRa_channel_code, payload_size, bandwidth);\n",
    "                     \n",
    "        # increment the send counter for the node\n",
    "        node.sent = node.sent + 1\n",
    "        \n",
    "        # increment the global sequence number for the frame\n",
    "        packetSeq = packetSeq + 1\n",
    "\n",
    "        # increment the local sequence number for the frame at the sending node\n",
    "        node.frame_seqnum = node.frame_seqnum + 1        \n",
    "\n",
    "        # create the packet\n",
    "        node.packet = []\n",
    "        \n",
    "        for rcvr_id in range(0, numReceivers):\n",
    "            dist = np.sqrt((node.x - rx_nodes[rcvr_id].x)*(node.x - rx_nodes[rcvr_id].x) + (node.y - rx_nodes[rcvr_id].y)*(node.y - rx_nodes[rcvr_id].y))\n",
    "            node.packet.append(myPacket(node.id, node.sf, payload_size, dist, rcvr_id))\n",
    "                   \n",
    "        \n",
    "        # packet reception at gateway    \n",
    "        for rcvr_id in range(0, numReceivers):               \n",
    "            if (node in packetsAtDest[rcvr_id]):\n",
    "                print (\"ERROR: packet already in\")\n",
    "            else:\n",
    "                # adding packet if no collision\n",
    "                collision_flag = checkcollision(node.packet[rcvr_id])\n",
    "                packetsAtDest[rcvr_id].append(node)\n",
    "                node.packet[rcvr_id].addTime = env.now\n",
    "                node.packet[rcvr_id].seqNr = packetSeq\n",
    "\n",
    "        # packet reception time\n",
    "        yield env.timeout(airtime(node.sf, LoRa_channel_code, payload_size, bandwidth))\n",
    "                \n",
    "        # if packet did not collide, add it to the list of received packets\n",
    "        # unless it is already in\n",
    "        for rcvr_id in range(0, numReceivers):\n",
    "            if not node.packet[rcvr_id].detection_failure:\n",
    "                if node.packet[rcvr_id].collided == 0:\n",
    "                    if (rcvr_id == 0):\n",
    "                        node.delivered = node.delivered + 1\n",
    "                    \n",
    "                    if node.id == 0:\n",
    "                        rx_nodes[rcvr_id].total_rcvd = rx_nodes[rcvr_id].total_rcvd + 1\n",
    "                    \n",
    "                    packetsRecDest[rcvr_id].append(node.packet[rcvr_id].seqNr)\n",
    "                                                    \n",
    "                    if (recPackets):\n",
    "                        if (recPackets[-1] != node.packet[rcvr_id].seqNr):\n",
    "                            recPackets.append(node.packet[rcvr_id].seqNr)\n",
    "                    else:\n",
    "                        recPackets.append(node.packet[rcvr_id].seqNr)\n",
    "                                        \n",
    "        # complete packet has been received by base station\n",
    "        # can remove it\n",
    "        for rcvr_id in range(0, numReceivers):\n",
    "            if (node in packetsAtDest[rcvr_id]):\n",
    "                packetsAtDest[rcvr_id].remove(node)\n",
    "                # reset the packet\n",
    "                node.packet[rcvr_id].collided = 0\n",
    "                node.packet[rcvr_id].processed = 0\n",
    "\n",
    "        # Stop the session once all K+R fragments have been transmitted\n",
    "        if transmit_nodes[0].sent == tx_limit:\n",
    "            stop_event.succeed()  # This will stop env.run()\n",
    "            return        \n",
    "        \n",
    "        yield env.timeout(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc96495-e490-4ce0-9735-86ef89ca3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This function simulates the transfer of FL updates over a wireless link\n",
    "#\n",
    "\n",
    "def simulate_update_transfer(transmitter_coordinates, receiver_coordinates, interferer_coordinates, sf, frag_size, k, r, isdownlink):\n",
    "\n",
    "    # global stuff\n",
    "    global transmit_nodes, packetsAtDest, packetsRecDest, recPackets, lostPackets, packetSeq, Dest_start, rx_nodes\n",
    "    global env\n",
    "    global total_tx_all, total_rx_all\n",
    "    \n",
    "    nrNodes = len(interferer_coordinates) + 1  # No. of transmitters in the network (incldues the interferers and the device transmitting the FL update)\n",
    "    \n",
    "    if isdownlink:\n",
    "        numReceivers = len(receiver_coordinates)\n",
    "    else:\n",
    "        numReceivers = 1\n",
    "            \n",
    "    if full_simulation:\n",
    "        \n",
    "        transmit_nodes = []\n",
    "        packetsAtDest = []\n",
    "        env = simpy.Environment()\n",
    "        Dest_start = []\n",
    "        \n",
    "        packetSeq = 0 # Will be incremented by 1 whenever a frame is sent in the network \n",
    "        \n",
    "        # list of received packets\n",
    "        recPackets=[]\n",
    "        lostPackets = []\n",
    "        \n",
    "        # list of base stations \n",
    "        rx_nodes = []\n",
    "        \n",
    "        # list of packets at each base station, init with 0 packets\n",
    "        packetsAtDest = []\n",
    "        packetsRecDest = []\n",
    "        \n",
    "        # activate the gateways and the relays\n",
    "        for i in range(0, numReceivers):\n",
    "            b = myReceiver(i, receiver_coordinates[i])\n",
    "            rx_nodes.append(b)\n",
    "            packetsAtDest.append([])\n",
    "            packetsRecDest.append([])\n",
    "            Dest_start.append(1)\n",
    "\n",
    "        # The stop_event is to be triggered once the FL update sender transmits the necessary frames (i.e., k+r fragments)\n",
    "        stop_event = env.event()\n",
    "        \n",
    "        # activate the transmitters (FL update sender + interferers)\n",
    "        for i in range(0, nrNodes):\n",
    "            if i == 0:\n",
    "                coordinates = transmitter_coordinates\n",
    "            else:\n",
    "                coordinates = interferer_coordinates[i-1]\n",
    "            node = myTransmitter(i, coordinates, mean_interf_interval)\n",
    "            transmit_nodes.append(node)\n",
    "            env.process(transmit(env, node, frag_size, interf_payload_range, k + r, numReceivers, rx_nodes, stop_event))\n",
    "            \n",
    "        env.run(until = stop_event)\n",
    "    \n",
    "        success = []\n",
    "\n",
    "        # Keeping track of total FL frames sent across all devices (for debugging purposes)\n",
    "        total_tx_all += transmit_nodes[0].sent\n",
    "        total_rx_all += rx_nodes[0].total_rcvd\n",
    "        \n",
    "        for rcvr_id in range(numReceivers):\n",
    "            dist = np.sqrt((transmit_nodes[0].x-rx_nodes[rcvr_id].x)*(transmit_nodes[0].x-rx_nodes[rcvr_id].x)+(transmit_nodes[0].y-rx_nodes[rcvr_id].y)*(transmit_nodes[0].y-rx_nodes[rcvr_id].y))\n",
    "            #print(dist, transmit_nodes[0].sent, rx_nodes[rcvr_id].total_rcvd, float(total_rx_all)/float(total_tx_all), 'sf: ', sf, 'frag_size: ', frag_size, 'k: ', k, 'r: ', r, 'downlink: ',isdownlink)\n",
    "            #print(\"analytical: \",compute_fragment_delivery_prob([dist], sf, frag_size))\n",
    "            #print(\"fl_pkts: \",transmit_nodes[0].sent)\n",
    "            #print(\"interf_pkts: \",transmit_nodes[1].sent)\n",
    "            success.append(rx_nodes[rcvr_id].total_rcvd >= k)\n",
    "    \n",
    "    else:\n",
    "        success = []\n",
    "        mdp = compute_model_delivery_prob(transmitter_coordinates, receiver_coordinates, sf, frag_size, k, r)\n",
    "\n",
    "        for rcvr_id in range(numReceivers):                \n",
    "            r_val = np.random.rand()\n",
    "            success.append(r_val <= mdp[rcvr_id])\n",
    "            \n",
    "    return success\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5ec57-9cc5-4ed2-ad86-135d2dff27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Simulate a federated learning session based on parameters specified in the main() function\n",
    "#\n",
    "\n",
    "import random\n",
    "def start_simulation(session_num, total_rounds, num_clients, network_radius, duty_cycle_percentage, sampling_ratio, use_quantization, use_sparsity, sparsity_threshold, apply_zlib, quantization_bits, ping_slot_duration, spreading_factor_FL, FEC_rate, LoRa_Class, trainloaders, testloader):\n",
    "    \n",
    "    global global_accuracy, global_loss, prediction_accuracy, prediction_loss, precision, recall, f1\n",
    "    global simulation_radius, num_interferers, avg_num_interferers\n",
    "    \n",
    "    compress_global_update = True\n",
    "    \n",
    "    compression_ratios = []\n",
    "    sparsity_levels = []\n",
    "    active_clients_indices = []\n",
    "    client_update_sizes = []                #Track size of each clients update\n",
    "    total_uplink_airtime = []               #Track total communication cost on the uplink\n",
    "    round_completion_time = [];\n",
    "    total_client_updates_delivered = [];\n",
    "    client_distances = [];\n",
    "    client_coordinates = [];\n",
    "    interferer_distances = [];\n",
    "    interferer_coordinates = [];\n",
    "    \n",
    "    downlink_airtime = [0] * total_rounds\n",
    "    slotted_downlink_airtime = [0] * total_rounds \n",
    "    max_uplink_airtime = [0] * total_rounds \n",
    "    downlink_start_time = [0] * total_rounds\n",
    "    downlink_end_time = [0] * total_rounds\n",
    "    uplink_start_time = [0] * total_rounds\n",
    "    uplink_end_time = [0] * total_rounds\n",
    "\n",
    "    \n",
    "    processing_delay = 0 # time (in seconds) needed by a client to perform a local update\n",
    "    \n",
    "    print(\"\\n---------------------------\\n    Starting Session\",  session_num, \"    \\n---------------------------\")\n",
    "\n",
    "    # Assign random locations to the clients (uniform distribution over a circle)\n",
    "    for client_idx in range(num_clients):\n",
    "        r1 = random.uniform(0, 1)\n",
    "        dist = math.sqrt(r1) * network_radius                \n",
    "        theta = random.uniform(0, 2*3.14)        \n",
    "        x_coord = dist * math.cos(theta)\n",
    "        y_coord = dist * math.sin(theta) \n",
    "        client_distances.append(dist)\n",
    "        client_coordinates.append([x_coord, y_coord])\n",
    "        \n",
    "    #print('Client distances from server (in meters):', client_distances) \n",
    "\n",
    "\n",
    "    # Generate a random number of interferers according to a Poisson Point Process\n",
    "    interference_radius = 3000\n",
    "    simulation_radius = network_radius + interference_radius\n",
    "    avg_num_interferers = np.pi * (simulation_radius**2) * interferer_intensity\n",
    "    \n",
    "    num_interferers = np.random.poisson(avg_num_interferers)\n",
    "    \n",
    "    for interf_idx in range(num_interferers):\n",
    "        \n",
    "        # Assign interferer locations (uniform distribution over a circle)\n",
    "        r1 = random.uniform(0, 1)\n",
    "        dist = math.sqrt(r1) * simulation_radius        \n",
    "        \n",
    "        theta = random.uniform(0, 2*3.14)        \n",
    "        x_coord = dist * math.cos(theta)\n",
    "        y_coord = dist * math.sin(theta) \n",
    "        interferer_distances.append(dist)\n",
    "        interferer_coordinates.append([x_coord, y_coord]) \n",
    "\n",
    "    # The gateway is at the center\n",
    "    gateway_coordinates = [0,0]\n",
    "    \n",
    "    # Initialize models\n",
    "    try:\n",
    "        global_model = Net().to(DEVICE)\n",
    "        client_models = [Net().to(DEVICE) for _ in range(num_clients)]\n",
    "        global_model_at_client = Net().to(DEVICE)\n",
    "        \n",
    "        # Initialize random number generators\n",
    "        torch.manual_seed(int(time.time()))\n",
    "        np.random.seed(int(time.time()))\n",
    "        \n",
    "        for model in [global_model] + [global_model_at_client]:\n",
    "            for param in model.parameters():\n",
    "                if param.requires_grad:\n",
    "                    if len(param.data.shape) >= 2:  # Weights (2D or higher)\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    else:  # Biases (1D)\n",
    "                        torch.nn.init.uniform_(param.data, a=-0.1, b=0.1)\n",
    "\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"Error: 'Net' or 'DEVICE' not defined. Ensure model and device are set up: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing models: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "            \n",
    "    for round in range(total_rounds):\n",
    "        \n",
    "        # Randomly sample a subset of clients for participation in this round\n",
    "        subset_size = np.ceil(sampling_ratio * num_clients)\n",
    "        sampled_clients = random.sample(range(num_clients), int(subset_size));\n",
    "        sampled_clients.sort()\n",
    "        \n",
    "        # Variables to store total on-air times and the local update-delivery statistics in this round\n",
    "        uplink_airtime = 0\n",
    "        per_client_uplink_airtime = [0] * num_clients # Total airtime spent by the clients in this session\n",
    "        \n",
    "        total_updates_delivered = 0\n",
    "        \n",
    "        accuracy = []\n",
    "        local_loss = []\n",
    "        round_sizes= []\n",
    "        round_sparsity= []\n",
    "        round_compression= []\n",
    "\n",
    "        compressed_updates= []\n",
    "        shapes_list= []\n",
    "        min_max_list= []\n",
    "\n",
    "        clients_to_tx = [] # Clients that are going to transmit their local updates to the server in this round\n",
    "        local_updates_delivered = [] # Clients that successfully delivered their local updates to the master in this round\n",
    "        \n",
    "        if round == 0:\n",
    "            # In the intial round, every client has the same random model parameters\n",
    "            # Global model assumed to be generated locally at each device (using same random seed, so that everyone has the same model)\n",
    "            # No downlink transmissions needed\n",
    "            updated_clients = range(num_clients)\n",
    "            global_model_at_client = copy.deepcopy(global_model)\n",
    "            downlink_airtime[round] = 0 \n",
    "\n",
    "            # Although the model is not actually transmitted, server still computes how long it will take to transmit it on the uplink\n",
    "            # The result will be used to calculate when the next round should start such that duty cycle restrictions are satsified\n",
    "            if compress_global_update:\n",
    "                compressed_global_update, compressed_global_update_size, shapes, min_max, comp_ratio = compress_update(global_model, None, use_sparsity, use_quantization, apply_zlib, sparsity_threshold, quantization_bits)\n",
    "                update_size_bytes = compressed_global_update_size * 1024  # convert kilobytes to bytes\n",
    "            else:\n",
    "                update_size_bytes = get_model_size(global_model) * 1024\n",
    "            \n",
    "            frag_size = LoRa_MTU(spreading_factor_FL)\n",
    "            k0 = np.ceil(float(update_size_bytes) / float(frag_size))\n",
    "            r0 = np.ceil(float(k0) / float(FEC_rate)) - k0 \n",
    "            airtime0 = (k0 + r0) * LoRa_frame_duration(spreading_factor_FL, frag_size)\n",
    "        \n",
    "        else:\n",
    "            if compress_global_update:\n",
    "                                \n",
    "                compressed_global_update, compressed_global_update_size, shapes, min_max, comp_ratio = compress_update(global_model, None, use_sparsity, use_quantization, apply_zlib, sparsity_threshold, quantization_bits)\n",
    "                downlink_update_size_bytes = compressed_global_update_size * 1024  # convert kilobytes to bytes\n",
    "    \n",
    "                # The decompressed version of the global model that the client will retrieve\n",
    "                update_dicts_global = []\n",
    "                for i, (comp_update, shapes, min_max) in enumerate(zip([compressed_global_update], [shapes], [min_max])):\n",
    "                    update_dict_global = decompress_update(compressed_global_update, shapes, min_max, use_quantization = use_quantization, apply_zlib = apply_zlib, quantization_bits = quantization_bits)\n",
    "                    update_dicts_global.append(update_dict_global)\n",
    "                aggregated_global_update = aggregate_updates(update_dicts_global)\n",
    "                with torch.no_grad():\n",
    "                    for name, param in global_model_at_client.named_parameters():\n",
    "                        if name in aggregated_global_update:\n",
    "                            param.data.copy_(aggregated_global_update[name].to(param.device))\n",
    "    \n",
    "            else:\n",
    "                global_model_at_client = copy.deepcopy(global_model) # No compression; so client get the exact same model as the server\n",
    "                downlink_update_size_bytes = get_model_size(global_model) * 1024 # convert kilobytes to bytes\n",
    "    \n",
    "                \n",
    "            # Transmission parameters to be used for server-to-client communications\n",
    "            downlink_sf = spreading_factor_FL\n",
    "            downlink_frag_size = LoRa_MTU(downlink_sf)\n",
    "            downlink_k = np.ceil(float(downlink_update_size_bytes) / float(downlink_frag_size))\n",
    "            downlink_r = np.ceil(float(downlink_k) / float(FEC_rate)) - downlink_k \n",
    "                    \n",
    "            # Total airtime incurred on the downlink in this round\n",
    "            downlink_airtime[round] = (downlink_k + downlink_r) * LoRa_frame_duration(downlink_sf, downlink_frag_size)\n",
    "\n",
    "            # Find the clients that received the global update\n",
    "            sampled_client_coordinates = [] \n",
    "            updated_clients = []\n",
    "            \n",
    "            for client_no in sampled_clients:\n",
    "                sampled_client_coordinates.append(client_coordinates[client_no])\n",
    "                \n",
    "            global_model_delivered = simulate_update_transfer(gateway_coordinates, sampled_client_coordinates, interferer_coordinates, downlink_sf, downlink_frag_size, downlink_k, downlink_r, isdownlink = True)\n",
    "            for i in range(len(sampled_clients)):     \n",
    "                if global_model_delivered[i]:            \n",
    "                    updated_clients.append(sampled_clients[i])\n",
    "\n",
    "            #print(\"Sampled clients: \", sampled_clients)\n",
    "            #print(\"Global update delivered to: \", updated_clients)\n",
    "            \n",
    "        # The exisiting local models which are to be updated\n",
    "        local_models_to_update = [client_models[i] for i in updated_clients if i < len(client_models)]\n",
    "\n",
    "        # Clients that will transmit local model in this round\n",
    "        for i in range(num_clients):\n",
    "            if i in sampled_clients and i in updated_clients:\n",
    "                clients_to_tx.append(i)\n",
    "\n",
    "        # Downlink transmission time in this round\n",
    "        if LoRa_Class == 'B':\n",
    "            # Total no. of ping slots occupied by the downlink transmission multiplied by ping slot duration  \n",
    "            slotted_downlink_airtime[round] = np.ceil(float(downlink_airtime[round]) / float(ping_slot_duration))  * ping_slot_duration\n",
    "\n",
    "        # Total time that has elapsed from the session's beginning till this point (i.e., the end of this downlink transmission)\n",
    "        if round == 0:\n",
    "            # In round 0, there was no downlink transmission, so time needed\n",
    "            downlink_start_time[round] = 0\n",
    "            downlink_end_time[round] = 0\n",
    "        \n",
    "        elif round == 1:\n",
    "            # Assumptions: \n",
    "            # All clients transmit simultaneously  --> Round 0 ended as soon as the longest uplink transfer ended\n",
    "            # We will wait till time t0, such that longest_uplink_transfer/t0 < duty_cycle            \n",
    "            if LoRa_Class == 'C':\n",
    "                downlink_start_time[round] = airtime0 * 100.0 / float(duty_cycle_percentage)\n",
    "                downlink_end_time[round] = downlink_start_time[round] + downlink_airtime[round]\n",
    "            else:\n",
    "                slots = np.ceil(float(airtime0) / float(ping_slot_duration))\n",
    "                downlink_start_time[round] = slots * ping_slot_duration * 100.0 / float(duty_cycle_percentage)\n",
    "                downlink_end_time[round] = downlink_start_time[round] + slotted_downlink_airtime[round]\n",
    "        \n",
    "        else:\n",
    "            if LoRa_Class == 'C':\n",
    "                downlink_start_time[round] = downlink_start_time[round - 1] + downlink_airtime[round - 1] * 100.0 / float(duty_cycle_percentage)\n",
    "                downlink_end_time[round] = downlink_start_time[round] + downlink_airtime[round]\n",
    "            else:\n",
    "                downlink_start_time[round] = downlink_start_time[round - 1] + slotted_downlink_airtime[round - 1] * 100.0 / float(duty_cycle_percentage)\n",
    "                downlink_end_time[round] = downlink_start_time[round] + slotted_downlink_airtime[round]\n",
    "                        \n",
    "        \n",
    "        if not updated_clients:\n",
    "            print(f\"\\nSkipping aggregation in Round {round} as no client received global update.\\n\")\n",
    "\n",
    "            prediction_accuracy.append(prediction_accuracy[-1])\n",
    "            prediction_loss.append(prediction_loss[-1])\n",
    "            precision.append(precision[-1])\n",
    "            recall.append(recall[-1])\n",
    "            f1.append(f1[-1])\n",
    "            total_uplink_airtime.append(0)\n",
    "            round_completion_time.append(downlink_end_time[round])\n",
    "            total_client_updates_delivered.append(0)\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        for client_idx, model in zip(updated_clients, local_models_to_update):\n",
    "            \n",
    "            set_parameters_to_clients(model, global_model_at_client)      # The client now has the latest global model parameters\n",
    "            train(model, trainloaders[client_models.index(model)], accuracy, local_loss, epochs = 1)  # Train client model\n",
    "\n",
    "            if client_idx in clients_to_tx:\n",
    "                \n",
    "                # Sparsify, quantize, and compress the update\n",
    "                compressed_update, update_size, shapes, min_max, comp_ratio = compress_update(model, global_model_at_client, use_sparsity, use_quantization, apply_zlib, sparsity_threshold, quantization_bits)\n",
    "                update_size_bytes = update_size * 1024\n",
    "                \n",
    "                # Spreading factor, fragment size, and redundancy allocation\n",
    "                uplink_sf = spreading_factor_FL\n",
    "                uplink_frag_size = LoRa_MTU(uplink_sf)\n",
    "                uplink_k = np.ceil(float(update_size_bytes) / float(uplink_frag_size))\n",
    "                uplink_r = np.ceil(float(uplink_k) / float(FEC_rate)) - uplink_k \n",
    "    \n",
    "                UL_transfer_success = simulate_update_transfer(client_coordinates[client_idx], [gateway_coordinates], interferer_coordinates, uplink_sf, uplink_frag_size, uplink_k, uplink_r, isdownlink = False)\n",
    "                \n",
    "                if UL_transfer_success[0]:\n",
    "                    uplink_update_delivered = True\n",
    "                    total_updates_delivered += 1\n",
    "                    local_updates_delivered.append(client_idx)\n",
    "                else:\n",
    "                    uplink_update_delivered = False\n",
    "                            \n",
    "                # Increment communication cost on the uplink\n",
    "                uplink_airtime += (uplink_k + uplink_r) * LoRa_frame_duration(uplink_sf, uplink_frag_size)\n",
    "\n",
    "                per_client_uplink_airtime[client_idx] = (uplink_k + uplink_r) * LoRa_frame_duration(uplink_sf, uplink_frag_size)\n",
    "                \n",
    "                if uplink_update_delivered:\n",
    "                    compressed_updates.append(compressed_update)\n",
    "                    shapes_list.append(shapes)\n",
    "                    min_max_list.append(min_max if use_quantization else {})\n",
    "                    round_sizes.append(update_size)\n",
    "                    round_compression.append(comp_ratio)\n",
    "\n",
    "        max_uplink_airtime[round] = np.max(per_client_uplink_airtime)\n",
    "        \n",
    "        # Update the elapsed time \n",
    "        if LoRa_Class == 'C':\n",
    "            uplink_start_time[round] = downlink_end_time[round] + processing_delay\n",
    "            uplink_end_time[round] = uplink_start_time[round] + max_uplink_airtime[round]\n",
    "        else:\n",
    "            uplink_start_time[round] = downlink_end_time[round] + np.ceil(float(processing_delay) / float(ping_slot_duration)) * ping_slot_duration\n",
    "            uplink_end_time[round] = uplink_start_time[round] + np.ceil(float(max_uplink_airtime[round]) / float(ping_slot_duration)) * ping_slot_duration\n",
    "        \n",
    "        #print(\"Local updates received from: \", local_updates_delivered)\n",
    "        \n",
    "        client_update_sizes.append(round_sizes)\n",
    "        sparsity_levels.append(statistics.mean(round_sparsity) if round_sparsity else 0.0)\n",
    "        compression_ratios.append(statistics.mean(round_compression) if round_compression else 1.0)\n",
    "        \n",
    "        global_accuracy.append(statistics.mean(accuracy) if accuracy else 0.0)\n",
    "        global_loss.append(statistics.mean(local_loss) if local_loss else 0.0)\n",
    "        \n",
    "        # Decompress updates\n",
    "        update_dicts = []\n",
    "        for i, (comp_update, shapes, min_max) in enumerate(zip(compressed_updates, shapes_list, min_max_list)):\n",
    "            update_dict = decompress_update(comp_update, shapes, min_max, use_quantization = use_quantization, apply_zlib = apply_zlib, quantization_bits = quantization_bits)\n",
    "            update_dicts.append(update_dict)\n",
    "    \n",
    "\n",
    "        # Aggregate updates\n",
    "        aggregated_update = aggregate_updates(update_dicts)\n",
    "\n",
    "        # Updates the global model\n",
    "        with torch.no_grad():\n",
    "            for name, param in global_model.named_parameters():\n",
    "                if name in aggregated_update:\n",
    "                    param.data += aggregated_update[name].to(param.device)\n",
    "        \n",
    "        # Test global model\n",
    "        pred_loss, pred_accuracy, y_true, y_pred = test(global_model, testloader)\n",
    "        prediction_accuracy.append(pred_accuracy)\n",
    "        prediction_loss.append(pred_loss)\n",
    "        prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1_score_val = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        f1.append(f1_score_val)\n",
    "        total_uplink_airtime.append(uplink_airtime)\n",
    "        round_completion_time.append(uplink_end_time[round])\n",
    "        total_client_updates_delivered.append(total_updates_delivered)\n",
    "        \n",
    "        print(f\"\\nResults for Session {session_num}, Round {round}:\\n\\tSampled clients: {sampled_clients} \\n\\tGlobal update received by {updated_clients} \\n\\tLocal updates delivered by {local_updates_delivered} \\n\\tround_completion_time = {round_completion_time[round] } \\n\\tdownlink_airtime = {downlink_airtime[round]} \\n\\ttotal uplink_airtime = {total_uplink_airtime[round]} \\n\\tAccuracy {pred_accuracy} \\n\\tLoss {pred_loss} \\n\\tprecision_score = {prec} \\n\\trecall_score = {rec} \\n\\tF1_score = {f1_score_val}\")\n",
    "        \n",
    "    return client_update_sizes, total_uplink_airtime, downlink_airtime, round_completion_time , total_client_updates_delivered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a237cf8-9f3f-4781-b9f0-aedd2c3323b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The Main Program: Federated Learning With Long Range (LoRa) Comunications\n",
    "#\n",
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    global global_accuracy, global_loss, prediction_accuracy\n",
    "    global prediction_loss, precision, recall, f1    \n",
    "    global full_simulation\n",
    "    global total_tx_all \n",
    "    global total_rx_all \n",
    "    global interferer_intensity, simulation_radius\n",
    "    global mean_interf_interval, interf_payload_range, LoRa_channel_code, bandwidth, Ptx, path_loss_exp \n",
    "    global spreading_factor_FL, num_channels, sensitivity, cap_thresh, GtGr, d0, Lpld0\n",
    "\n",
    "            \n",
    "    all_prediction_accuracies = []\n",
    "    all_airtime_uplink = []\n",
    "    all_airtime_downlink = []\n",
    "    all_completion_time = []\n",
    "    all_client_updates_delivered = []\n",
    "    execution_time = []\n",
    "\n",
    "    # Number of FL sessions to simulate\n",
    "    num_sessions = 20\n",
    "\n",
    "    # Number of training rounds per session   \n",
    "    rounds_per_session = 15\n",
    "    \n",
    "    # Number of clients\n",
    "    num_clients = 20\n",
    "\n",
    "    # Network radius in meters (clients will be randomly deployed within a circle of this radius, with the server at the origin)\n",
    "    network_radius = 500\n",
    "\n",
    "    # Spreading factor to be used for FL update transfers\n",
    "    spreading_factor_FL = 9\n",
    "    \n",
    "    # Whether to quantize the parameter values, and if so, to how many bits (supports 1, 2, and 4)\n",
    "    # If False, then 32-bit reprentation will be used\n",
    "    use_quantization = True\n",
    "    \n",
    "    if use_quantization:\n",
    "        quantization_bits = 4\n",
    "    else:\n",
    "        quantization_bits = 0\n",
    "\n",
    "    # Whether to apply sparsity (set parameter values below a threshold to zero), and if so, what threshold to use\n",
    "    use_sparsity = True\n",
    "    if use_sparsity:\n",
    "        sparsity_threshold = 0.001\n",
    "    else:\n",
    "        sparsity_threshold = 0\n",
    "\n",
    "    # Whether to compress the updates using zlib compression prior to transmission\n",
    "    apply_zlib = True\n",
    "\n",
    "    # The class of LoRaWAN operation to be used (supports class B and C)\n",
    "    LoRa_Class = 'B'\n",
    "        \n",
    "    # Ping slot periodicity (only for Class B)\n",
    "    ping_slot_duration = 0.03\n",
    "    \n",
    "    # Sampling ratio (the fraction of clients chosen for update exchange in each round)\n",
    "    sampling_ratio = 0.4\n",
    "        \n",
    "    # Rate of the FEC applied to the fragments\n",
    "    FEC_rate = 0.5\n",
    "\n",
    "    # Maximum permitted value for transmitter duty cycle (in percentage)\n",
    "    duty_cycle_percentage = 1.0\n",
    "\n",
    "    # No. of interferers per sq. meter (assumes a Poisson Point Process)\n",
    "    interferer_intensity = 10.0**(-4.0)\n",
    "    \n",
    "    # Whether to actually simulate LoRa tranmissions or apply analytical approximations\n",
    "    full_simulation = False\n",
    "\n",
    "    # Mean of the time interval (in milliseconds) between two consecutive packets from an interferer\n",
    "    mean_interf_interval = 60000 \n",
    "\n",
    "    # Range of values for interfering frame payload (an interfering frame carries a payload that is chosen uniformly at random from within this range)             \n",
    "    interf_payload_range = range(1,50)  \n",
    "\n",
    "    # Channel coding rate for LoRa frames\n",
    "    LoRa_channel_code = 1 \n",
    "\n",
    "    # Transmission bandwidth (kHz)\n",
    "    bandwidth = 125 \n",
    "\n",
    "    # power of the transmitted signal in dBm\n",
    "    Ptx = 14 \n",
    "\n",
    "    # Path loss exponent\n",
    "    path_loss_exp = 2.5 \n",
    "\n",
    "    # Number of non overlapping frequency bands available to LoRa transmitters\n",
    "    num_channels = 8 \n",
    "    \n",
    "    # LoRa receiver sensitivities (for spreading factors 7 through 12) \n",
    "    sensitivity = [-123, -126, -129, -132, -134.5, -137]\n",
    "    \n",
    "    # capture thresholds \n",
    "    cap_thresh = np.array([[1, -8, -9, -9, -9, -9],\n",
    "                           [-11, 1, -11, -12, -13, -13],\n",
    "                           [-15, -13, 1, -13, -14, -15],\n",
    "                           [-19, -18, -17, 1, -17, -18],\n",
    "                           [-22, -22, -21, -20, 1, -20],\n",
    "                           [-25, -25, -25, -24, -23, 1]])\n",
    "        \n",
    "    # Product of tx and rx antenna gains (dB)  \n",
    "    GtGr = 6\n",
    "\n",
    "    # Reference distance (for LoRa signal strength computation)\n",
    "    d0 = 40.0\n",
    "\n",
    "    # Path loss (dB) at reference distance\n",
    "    Lpld0 = 127.41\n",
    "\n",
    "    # Matrices to store outputs\n",
    "    all_prediction_accuracies = []\n",
    "    all_airtime_uplink = []\n",
    "    all_airtime_downlink = []\n",
    "    all_completion_time = []\n",
    "    all_client_updates_delivered = []\n",
    "    execution_time = []\n",
    "\n",
    "    # For keeping track of total transmissions and receptions (for debugging purposes)\n",
    "    total_tx_all = 0\n",
    "    total_rx_all = 0\n",
    "    \n",
    "    # Capture timestamp for the simulation (in IST)\n",
    "    ist = pytz.timezone('Asia/Kolkata')\n",
    "    simulation_timestamp = datetime.now(ist).strftime(\"%I:%M %p IST, %A, %B %d, %Y\")\n",
    "    \n",
    "    trainloaders, valloaders, testloader, datasets = load_datasets(num_clients)\n",
    "    \n",
    "    for session in range(num_sessions):\n",
    "        \n",
    "        # Reset global variables\n",
    "        global_accuracy = []\n",
    "        global_loss = []\n",
    "        prediction_accuracy = []\n",
    "        prediction_loss = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        f1 = []\n",
    "\n",
    "        start_time = time.time()\n",
    "                \n",
    "\n",
    "        # Run simulation\n",
    "        client_update_sizes, total_uplink_airtime, total_airtime_downlink, round_completion_time , client_updates_delivered  = start_simulation(session, total_rounds = rounds_per_session, num_clients = num_clients,\n",
    "                                                                   network_radius = network_radius, duty_cycle_percentage = duty_cycle_percentage, sampling_ratio = sampling_ratio, use_quantization = use_quantization,use_sparsity = use_sparsity, sparsity_threshold = sparsity_threshold, apply_zlib = apply_zlib, quantization_bits = quantization_bits, ping_slot_duration = ping_slot_duration, spreading_factor_FL = spreading_factor_FL, FEC_rate = FEC_rate, LoRa_Class = LoRa_Class, trainloaders= trainloaders, testloader= testloader)\n",
    "        # Collect results\n",
    "        all_prediction_accuracies.append(prediction_accuracy)\n",
    "        all_airtime_uplink.append(total_uplink_airtime)\n",
    "        all_airtime_downlink.append(total_airtime_downlink)\n",
    "        all_completion_time.append(round_completion_time )\n",
    "        all_client_updates_delivered.append(client_updates_delivered)\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time.append(end_time - start_time)\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    rounds= list(range(1, rounds_per_session + 1))\n",
    "\n",
    "    for exp, prediction_accuracy in enumerate(all_prediction_accuracies):           \n",
    "        plt.plot(rounds, prediction_accuracy, label=f'Session {exp+1}', marker= 'o')\n",
    "\n",
    "    \n",
    "    #Compute averages  \n",
    "    average_accuracy = np.mean(all_prediction_accuracies, axis = 0)\n",
    "    average_uplink_time = np.mean(all_airtime_uplink, axis = 0)\n",
    "    average_downlink_time = np.mean(all_airtime_downlink, axis = 0)\n",
    "    average_completion_time = np.mean(all_completion_time, axis = 0)    \n",
    "    average_client_updates_delivered = np.mean(all_client_updates_delivered, axis = 0)\n",
    "    average_simulation_time = np.mean(execution_time)\n",
    "    \n",
    "    # print averaged outputs\n",
    "    print('\\n average accuracy: ', average_accuracy)\n",
    "    print('average completion time: ', average_completion_time)\n",
    "    print('average uplink time: ', average_uplink_time)\n",
    "    print('average downlink time: ', average_downlink_time)\n",
    "    print('average client_updates delivered: ', average_client_updates_delivered)\n",
    "    print('simulation time per session: ', average_simulation_time)\n",
    "    \n",
    "    # Write results to a file\n",
    "    filename = \"Output.txt\"\n",
    "    def write_results(filename, spreading_factor_FL, FEC_rate, average_accuracy, average_uplink_time, average_downlink_time, average_completion_time, average_client_updates_delivered):\n",
    "        with open(filename, \"a\") as file:\n",
    "            file.write(f\"Simulation Run Timestamp: {simulation_timestamp}\\n\")\n",
    "            file.write(\"=\" * 36 + \"\\n\")\n",
    "            # Write header with parameters\n",
    "            file.write(\"Federated Learning Simulation Results\\n\")\n",
    "            file.write(\"=\" * 36 + \"\\n\")\n",
    "            file.write(f\"Spreading factor: {spreading_factor_FL}\\n\")\n",
    "            file.write(f\"FEC rate: {FEC_rate}\\n\")\n",
    "            file.write(f\"Number of FL sessions: {num_sessions}\\n\")\n",
    "            file.write(f\"Number of training rounds per experiment: {rounds_per_session}\\n\")\n",
    "            file.write(f\"Number of clients: {num_clients}\\n\")\n",
    "            file.write(f\"Network radius (in meters): {network_radius}\\n\")\n",
    "            file.write(f\"Interferer intensity: {interferer_intensity}\\n\")\n",
    "            file.write(f\"Mean interference interval (milliseconds): {mean_interf_interval}\\n\")\n",
    "            file.write(f\"Quantization bits: {quantization_bits}\\n\")\n",
    "            file.write(f\"Sparsity threshold: {sparsity_threshold}\\n\")\n",
    "            file.write(f\"LoRa Class of Operation: {LoRa_Class}\\n\")\n",
    "            file.write(f\"Ping slot periodicity (in milliseconds): {ping_slot_duration}\\n\")\n",
    "            file.write(f\"Sampling ratio: {sampling_ratio}\\n\")\n",
    "            file.write(f\"Transmitter duty-cycle percentage: {duty_cycle_percentage}\\n\\n\")\n",
    "            file.write(f\"Full_simulation: {full_simulation}\\n\\n\")\n",
    "            \n",
    "            # Write table header with fixed-width columns\n",
    "            file.write(\"| {:<5} | {:>12} | {:>19} | {:>20} | {:>19} | {:>23} |\\n\".format(\n",
    "                \"Round\", \"Accuracy (%)\", \"Uplink Airtime (ms)\", \"Downlink Airtime (ms)\", \"Completion Time (ms)\", \"Client Updates Delivered\"\n",
    "            ))\n",
    "            file.write(\"| {:<5} | {:>12} | {:>19} | {:>20} | {:>19} | {:>23} |\\n\".format(\n",
    "                \"-\" * 5, \"-\" * 12, \"-\" * 19, \"-\" * 20, \"-\" * 19, \"-\" * 23\n",
    "            ))\n",
    "            \n",
    "            # Write table rows\n",
    "            for round_idx in range(len(average_accuracy)):\n",
    "                file.write(\"| {:<5} | {:>12.2f} | {:>19.2f} | {:>20.2f} | {:>19.2f} | {:>23.2f} |\\n\".format(\n",
    "                    round_idx + 1,\n",
    "                    average_accuracy[round_idx] * 100,  # Convert to percentage\n",
    "                    average_uplink_time[round_idx],\n",
    "                    average_downlink_time[round_idx],\n",
    "                    average_completion_time[round_idx],\n",
    "                    average_client_updates_delivered[round_idx]\n",
    "                ))\n",
    "            \n",
    "            # Add notes\n",
    "            file.write(\"\\nNotes:\\n\")\n",
    "            file.write(\"- Accuracy: Percentage of correct predictions.\\n\")\n",
    "            file.write(\"- Uplink Airtime: Time spent by clients in transmitting local updates (s).\\n\")\n",
    "            file.write(\"- Downlink Airtime: Time spent by the server in transmitting the global update (s).\\n\")\n",
    "            file.write(\"- Completion Time: Time elapsed till round completion (s).\\n\")\n",
    "            file.write(\"- Client Updates Delivered: Average number of clients that successfully delivered their local update to the server.\\n\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    \n",
    "    plt.plot(rounds, average_accuracy, label = 'Average Accuracy', color = 'black', linewidth = 2, linestyle ='--', marker= 'x')\n",
    "    write_results(filename, spreading_factor_FL, FEC_rate, average_accuracy, average_uplink_time, average_downlink_time, average_completion_time, average_client_updates_delivered )\n",
    "\n",
    "    filename=\"FedAvg_MNIST_LoRaWAN.png\"\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Prediction Accuracy')\n",
    "    plt.title('Average Prediction accuracy vs rounds')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    print(f\"Plot saved as {filename}\")\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8c316-affe-4588-9b14-ddd1bc431097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
